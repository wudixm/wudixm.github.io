[原文](https://blog.csdn.net/xiaoshunzi111/article/details/54312923)

**1. 字符串长度函数：length**

语法: length(string A)

返回值: int

说明：返回字符串A的长度

举例：

hive> select length('abcedfg') from lxw_dual;

7

**2. 字符串反转函数：reverse**

语法: reverse(string A)

返回值: string

说明：返回字符串A的反转结果

举例：

hive> select reverse(abcedfg') from lxw_dual;

gfdecba

**3. 字符串连接函数：concat**

语法: concat(string A, string B…)

返回值: string

说明：返回输入字符串连接后的结果，支持任意个输入字符串

举例：

hive> select concat('abc','def','gh') from lxw_dual;

abcdefgh

**4. 带分隔符字符串连接函数：concat_ws**

语法: concat_ws(string SEP, string A, string B…)

返回值: string

说明：返回输入字符串连接后的结果，SEP表示各个字符串间的分隔符

举例：

hive> select concat_ws(',','abc','def','gh') from lxw_dual;

abc,def,gh

**5. 字符串截取函数：substr,substring**

语法: substr(string A, int start),substring(string A, int start)

返回值: string

说明：返回字符串A从start位置到结尾的字符串

举例：

hive> select substr('abcde',3) from lxw_dual;

cde

hive> select substring('abcde',3) from lxw_dual;

cde

hive>  selectsubstr('abcde',-1) from lxw_dual;  （和ORACLE相同）

e

**6. 字符串截取函数：substr,substring**

语法: substr(string A, int start, int len),substring(string A, intstart, int len)

返回值: string

说明：返回字符串A从start位置开始，长度为len的字符串

举例：

hive> select substr('abcde',3,2) from lxw_dual;

cd

hive> select substring('abcde',3,2) from lxw_dual;

cd

hive>select substring('abcde',-2,2) from lxw_dual;

de

**7. 字符串转大写函数：upper,ucase**

语法: upper(string A) ucase(string A)

返回值: string

说明：返回字符串A的大写格式

举例：

hive> select upper('abSEd') from lxw_dual;

ABSED

hive> select ucase('abSEd') from lxw_dual;

ABSED

**8. 字符串转小写函数：lower,lcase**

语法: lower(string A) lcase(string A)

返回值: string

说明：返回字符串A的小写格式

举例：

hive> select lower('abSEd') from lxw_dual;

absed

hive> select lcase('abSEd') from lxw_dual;

absed

**9. 去空格函数：trim**

语法: trim(string A)

返回值: string

说明：去除字符串两边的空格

举例：

hive> select trim(' abc ') from lxw_dual;

abc

**10. 左边去空格函数：ltrim**

语法: ltrim(string A)

返回值: string

说明：去除字符串左边的空格

举例：

hive> select ltrim(' abc ') from lxw_dual;

abc

**11. 右边去空格函数：rtrim**

语法: rtrim(string A)

返回值: string

说明：去除字符串右边的空格

举例：

hive> select rtrim(' abc ') from lxw_dual;

abc

**12. 正则表达式替换函数：regexp_replace**

语法: regexp_replace(string A, string B, string C)

返回值: string

说明：将字符串A中的符合java正则表达式B的部分替换为C。注意，在有些情况下要使用转义字符,类似oracle中的regexp_replace函数。

举例：

hive> select regexp_replace('foobar', 'oo|ar', '') from lxw_dual;

fb

**13. 正则表达式解析函数：regexp_extract**

语法: regexp_extract(string subject, string pattern, int index)

返回值: string

说明：将字符串subject按照pattern正则表达式的规则拆分，返回index指定的字符。

举例：

hive> select regexp_extract('foothebar', 'foo(.*?)(bar)', 1) fromlxw_dual;

the

hive> select regexp_extract('foothebar', 'foo(.*?)(bar)', 2) fromlxw_dual;

bar

hive> select regexp_extract('foothebar', 'foo(.*?)(bar)', 0) fromlxw_dual;

foothebar

**注意，在有些情况下要使用转义字符，下面的等号要用双竖线转义，这是java正则表达式的规则。**

select data_field,

​     regexp_extract(data_field,'.*?bgStart\\=([^&]+)',1) as aaa,

​     regexp_extract(data_field,'.*?contentLoaded_headStart\\=([^&]+)',1) as bbb,

​     regexp_extract(data_field,'.*?AppLoad2Req\\=([^&]+)',1) as ccc

​     from pt_nginx_loginlog_st

​     where pt = '2012-03-26'limit 2;

**14. URL解析函数：parse_url**

语法: parse_url(string urlString, string partToExtract [, stringkeyToExtract])

返回值: string

说明：返回URL中指定的部分。partToExtract的有效值为：HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO.

举例：

hive> selectparse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'HOST') fromlxw_dual;

facebook.com

hive> selectparse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY','k1') from lxw_dual;

v1

**15. json解析函数：get_json_object**

语法: get_json_object(string json_string, string path)

返回值: string

说明：解析json的字符串json_string,返回path指定的内容。如果输入的json字符串无效，那么返回NULL。

举例：

hive> select get_json_object('{"store":

\>  {"fruit":\[{"weight":8,"type":"apple"},{"weight":9,"type":"pear"}],

\>   "bicycle":{"price":19.95,"color":"red"}

\>   },

\> "email":"amy@only_for_json_udf_test.net",

\>  "owner":"amy"

\> }

\> ','$.owner') from lxw_dual;

amy

**16. 空格字符串函数：space**

语法: space(int n)

返回值: string

说明：返回长度为n的字符串

举例：

hive> select space(10) from lxw_dual;

hive> select length(space(10)) from lxw_dual;

10

**17. 重复字符串函数：repeat**

语法: repeat(string str, int n)

返回值: string

说明：返回重复n次后的str字符串

举例：

hive> select repeat('abc',5) from lxw_dual;

abcabcabcabcabc

**18. 首字符ascii函数：ascii**

语法: ascii(string str)

返回值: int

说明：返回字符串str第一个字符的ascii码

举例：

hive> select ascii('abcde') from lxw_dual;

97

**19. 左补足函数：lpad**

语法: lpad(string str, int len, string pad)

返回值: string

说明：将str进行用pad进行左补足到len位

举例：

hive> select lpad('abc',10,'td') from lxw_dual;

tdtdtdtabc

**注意：与GP，ORACLE不同，pad 不能默认**

**20. 右补足函数：rpad**

语法: rpad(string str, int len, string pad)

返回值: string

说明：将str进行用pad进行右补足到len位

举例：

hive> select rpad('abc',10,'td') from lxw_dual;

abctdtdtdt

**21. 分割字符串函数: split**

语法:  split(string str, stringpat)

返回值:  array

说明: 按照pat字符串分割str，会返回分割后的字符串数组

举例：

hive> select split('abtcdtef','t') from lxw_dual;

["ab","cd","ef"]

**22. 集合查找函数:find_in_set**

语法: find_in_set(string str, string strList)

返回值: int

说明: 返回str在strlist第一次出现的位置，strlist是用逗号分割的字符串。如果没有找该str字符，则返回0

举例：

hive> select find_in_set('ab','ef,ab,de') from lxw_dual;

2

hive> select find_in_set('at','ef,ab,de') from lxw_dual;

0

### 群组

```hive
%hive
with a as ( 
select  a.date, a.group_id, a.user_id  from dw.idl_group_hot a join dw.dim_group b 
on a.group_id = b.group_id and a.user_id = b.user_id
where a.recommended=5 and date>='${开始日期}' and date<='${结束日期}' ) 

select date,count(group_id) as `存在自荐的群个数` from ( 
select date, group_id from a group by date, group_id order by date )  x 
group by date
order by date

%hive
with d as (
select a.group_id, a.user_id, a.* from dw.idl_group_after_recommend a 
    join dw.dim_group b on a.group_id = b.group_id and a.user_id = b.user_id join dw.dim_video c on a.video_id = c.id and a.user_id = c.user_id
    where a.date >= '2020-10-01' and a.date < '2020-10-20' and a.is_creator = 'true' 
)

select group_id, count(group_id) from d group by group_id order by count(group_id) desc




```

### spark

```spark
%spark
val df = spark.read.parquet("/idl/group_after_recommend").filter($"date">="2020-10-01" && $"date" <= "2020-10-19").filter($"is_creator"==="true")
val group = spark.read.parquet("/dim/group").select("user_id","group_id")
val video = spark.read.parquet("/dim/video").select("user_id","video_id")
val digest = spark.read.parquet("/idl/video_digest").select("user_id", "video_id")


val zone = spark.read.parquet("/odl/color_group/cv_group_zone_item").select($"obj_id".as("video_id"),$"group_id")

val rs = df.join(group,Seq("user_id","group_id")).join(video,Seq("user_id","video_id")).join(digest, Seq("user_id","video_id"))
.join(zone,Seq("video_id","group_id"),"left_anti")

val re = rs.groupBy("group_id").agg(count("group_id").as("count")).orderBy($"count".desc)
```

### hive

客户端埋点，看这个查出来的e 字段，是额外信息，

```
%hive
select * from `dw`.`idl_app_ops_event` where date = '${date}' and id = ${id} limit 20

53706030
```

当天的日志报到report_client 下，log_data 底下的a->d->id 是埋点id

```
{"ip":"223.104.64.136","log_data":{"a":[{"d":{"s":0,"e":{"itemId":"5763857","devicesId":"c9f2b559686c1c57","userId":"9024228","groupId":"10617","videoId":"161285869"},"id":53706026},"ts":1604286001298,"l":{"me":{"author_id":12401397,"video_id":"161285869","r":2},"s":"cn.colorv.modules.group.fragment.GroupWorkFragment","m":"cn.colorv.modules.main.ui.activity.VideoPlayWithCommentActivity_B"},"n":{"s":"cn.colorv.modules.material.fragment.GroupDynamicSpaceFragment","m":"cn.colorv.modules.group.activity.NewGroupZoneActivity"},"u":9024228,"t":6,"net":"4G"},],"d":"c9f2b559686c1c57","v":"5.39.3","dv":7,"model":"SNE-AL00","os":"Android"},"act":"app_ops"}
```

```hive
select act, group_id, uids from parquet.`/logs2/report_video/date=2020-10-27` where  act = 'send_group_prize_lv22' and group_id = 10750
select act, group_id, user_id, video_id from parquet.`/logs2/report_video/date=2020-11-05` where  act = 'share_video_group_zone' and group_id = 10963

```

```spark
%spark
val df = spark.read.option("mergeSchema", "false").parquet("/logs2/report_video/date=2020-11-09")
.filter($"act" === "group_after_recommend")
.filter($"act" === "share_video_group_zone")
.filter($"group_id" === "10876")

.withColumn("id", monotonically_increasing_id())
.select("id","user_id","group_id", "video_id","timestamp")
val df2 = spark.read.parquet("/idl/app_ops_event/date=2020-11-09")
// .filter($"date">="2020-11-01")
.filter($"id"==="53706030")
.select($"user_id",get_json_object($"e","$.groupId").as("group_id"),get_json_object($"e","$.videoId").as("video_id"),$"timestamp".as("timestamp2"))
.filter($"group_id" === "10876")


val rs = df.join(df2,Seq("user_id","group_id","video_id"),"left")


z.show(rs.limit(10))
```

```hive
%hive
select ev.timestamp, ev.id, ev.udid, ev.user_id, ev.item_id, ev.e 
from `dw`.`idl_app_ops_event` ev join ``
where ev.date <= '2020-11-06' and ev.date >= '2020-11-01' and ev.id = 53706030 and ev.user_id = 5597629  
order by ev.timestamp desc


select video_id, timestamp,mp4_path_pre,mp4_path_post,recommend from parquet.`/logs2/report_video/date=2020-11-10` where act = 'share_video_group_zone' and group_id = 10748 

```

```hive
%hive
with d as (
select a.group_id, a.user_id, a.* from dw.idl_group_after_recommend a 
    join dw.dim_group b on a.group_id = b.group_id and a.user_id = b.user_id join dw.dim_video c on a.video_id = c.id and a.user_id = c.user_id join dw.odl_group_zone_item d on d.obj_id = a.video_id and d.group_id = a.group_id and d.recommended = 5
    where a.date >= '2020-10-01'  and a.is_creator = 'true' and a.group_id='10925' 
)

select group_id, count(group_id) from d group by group_id order by count(group_id) desc

```

```hive
%hive
-- select count(1) from dw.idl_group_after_recommend where date >= '2020-11-01' and group_id = 11168

select count(1) from dw.idl_group_after_recommend where date >= "${date}" and group_id = "${group_id}"
```

### 聊天

```hive
%hive
select * from dw.idl_group_events where date >= '2020-10-09' and group_id = 10631 and kind = 'im_message' and user_id = 7170790 limit 10 
```
