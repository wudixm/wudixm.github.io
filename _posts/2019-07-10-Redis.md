## 命令

### ttl

当 key 不存在时，返回 -2 。 当 key 存在但没有设置剩余生存时间时，返回 -1 。 否则，以毫秒为单位，返回 key 的剩余生存时间。

注意：在 Redis 2.8 以前，当 key 不存在，或者 key 没有设置剩余生存时间时，命令都返回 -1 。

### 事务

示例以下示例说明了如何启动和执行Redis事务。

```
redis 127.0.0.1:6379> MULTI
OK
redis 127.0.0.1:6379> SET mykey "redis"
QUEUED
redis 127.0.0.1:6379> GET mykey
QUEUED
redis 127.0.0.1:6379> INCR visitors
QUEUED
redis 127.0.0.1:6379> EXEC
1) OK
2) "redis"
3) (integer) 1
```

### redis 事务命令

下表列出了与Redis事务相关的一些基本命令。
|序号|命令|说明|
|----|----|----|
|1 |discard |丢弃在MULTI之后发出的所有命令 |
|2|exec|执行MULTI后发出的所有命令|
|3|multi|标记事务块的开始|
|4|unwatch|取消 WATCH 命令对所有 key 的监视|
|5|WATCH key [key …]|监视给定的键以确定MULTI / EXEC块的执行|

### 事务中出现异常

发现只有在exec 之后，前面的具体事务操作中的异常才会打印出来

```sh
127.0.0.1:6379[1]> multi
OK
127.0.0.1:6379[1]> ZINCRBY score: topic:1 100 // 这个应该是 zincrb score: 100 topic:1
QUEUED
127.0.0.1:6379[1]> exec
1) (error) ERR value is not a valid float
127.0.0.1:6379[1]>
```

看redis 官方文档[原文](https://redis.io/topics/transactions)
redis 就是不支持事务

```sh
127.0.0.1:6379[1]> hgetall topic:1
 1) "like_count"
 2) "12"
 3) "user_id"
 4) "11174736"
 5) "name"
 6) "title"
 7) "comment_count"
 8) "0"
 9) "description"
10) "content"
127.0.0.1:6379[1]> multi
OK
127.0.0.1:6379[1]> hincrby topic:1 like_count 100   // 就算后面一个命令出错，但是这一句确实执行了
QUEUED
127.0.0.1:6379[1]> ZINCRBY score: topic:1 100   // 这一个命令出错，但是不影响其它命令执行
QUEUED
127.0.0.1:6379[1]> exec
1) (integer) 112
2) (error) ERR value is not a valid float
127.0.0.1:6379[1]> hgetall topic:1
 1) "like_count"
 2) "112"
 3) "user_id"
 4) "11174736"
 5) "name"
 6) "title"
 7) "comment_count"
 8) "0"
 9) "description"
10) "content"
127.0.0.1:6379[1]>
```

```
EXEC returned two-element Bulk string reply where one is an OK code and the other an -ERR reply. It's up to the client library to find a sensible way to provide the error to the user.

**It's important to note that even when a command fails, all the other commands in the queue are processed – Redis will not stop the processing of commands.**
```

#### Why Redis does not support roll backs? 

不支持回滚，原因有2：
1. 一般性的出错原因都是program 出错，语法错误，很容易在开发过程中发现并改正。
2. 内部实现简单，快速，不需要回滚

```
If you have a relational databases background, the fact that Redis commands can fail during a transaction, but still Redis will execute the rest of the transaction instead of rolling back, may look odd to you.

However there are good opinions for this behavior:

Redis commands can fail only if called with a wrong syntax (and the problem is not detectable during the command queueing), or against keys holding the wrong data type: this means that in practical terms a failing command is the result of a programming errors, and a kind of error that is very likely to be detected during development, and not in production.
Redis is internally simplified and faster because it does not need the ability to roll back.
An argument against Redis point of view is that bugs happen, however it should be noted that in general the roll back does not save you from programming errors. For instance if a query increments a key by 2 instead of 1, or increments the wrong key, there is no way for a rollback mechanism to help. Given that no one can save the programmer from his or her errors, and that the kind of errors required for a Redis command to fail are unlikely to enter in production, we selected the simpler and faster approach of not supporting roll backs on errors.
```

### LRU Cache算法以及在redis中的应用

[原文](https://zhuanlan.zhihu.com/p/40354122)

覆盖一个现存的块的时候会使用替换策略，替换策略有很多种，主要有：
- LRU - Least Recently Used
LRU是很常用的替换策略，通常的实现会有一个age counter（替换index）与每个数组S相关。这个counter最大值就是S，当一个set被访问到，那么比它低的counter就被置为0，其他set自增1。
- FIFO - First-In First-Out
先进先出策略。
- LFU – Least Frequently Used
很高效的算法，但很耗资源，通常不用。
- Round-robin
有一个指针指向将要被替换的行，当行被替换，指针就会自增1，指针是环形的。
- Random
随机策略，用于全相联高速缓存。每个时序Round-robin就要更新，而不是每个替换操作。

### linux中的文件描述符(file descriptor)和文件
[原文](https://www.jianshu.com/p/504a53c30c17)

linux为了实现一切皆文件的设计哲学，不仅将数据抽象成了文件，也将一切操作和资源抽象成了文件，比如说硬件设备，socket，磁盘，进程，线程等。
这样的设计将系统的所有动作都统一起来，实现了对系统的原子化操作，大大降低了维护和操作的难度，想想看，对于socket，硬件设备，我们只要读读写写文件就能对其进行操作是多么爽的一件事

那么在操作这些所谓的文件的时候，我们不可能没操作一次就要找一次名字吧，这样会耗费大量的时间和效率。咱们可以每一个文件操作一个索引，这样，要操作文件的时候，我们直接找到索引就可以对其进行操作了。我们将这个索引叫做文件描述符（file descriptor），简称fd，在系统里面是一个非负的整数。每打开或创建一个文件，内核就会向进程返回一个fd，第一个打开文件是0,第二个是1,依次递增。

我们平时说的命令如./test.sh>res2.log 2>&1就是将标准和错误的输出流重定向到log文件里面，通常情况下系统启动后会自动启动文件描述符号0,1,2，当然，你也可以关闭这几个文件描述符，比如关掉1,然后打开一个文件，那么到时候你使用代码中的printf就不会输出到终端，而是会输入到你打开的文件里面（这样在测试的时候免去了我们在代码里面写log的麻烦）

在python中可以用如下拿到fd，在linux下fd叫做文件描述符，在window下fd叫做句柄，所以这就说明了为啥在官方文档中fileno解释是Return the file descriptor or handle used by the connection.

在linux内核中通常会有个task_struct结构体来维护进程相关的表，叫进程控制块，这个块里面会有指针指向file_struct的结构体，称为文件描述表，文件描述符就是这个表的索引。

而这个file_struct会指向一个file的结构体，一般情况下，进程是没有办法直接访问文件的，只能通过文件描述表里面的文件描述符找到文件。file有几个主要的结构体成员，分别是count，file_operation和dentry（directory entry）。
count：这个是引用计数，像上面的pipe，还有fork，dup等的文件描述符可能会指向同一个file，比如现在有fd1和fd2，他们都指向了同一个文件，那么这个文件的计数就是2,要想关闭这个文件，close（fd1）是不能关掉的，因为这个时候计数为1,只有在计数为0的时候才算完全关闭
file_operation：这个指向的文件操作指针，file_operation里面包含了对文件操作的内核函数指针，他指向内核操作函数，比如说read，write，release，open，当然，不同的文件file_opertions有不同的操作，像读取字符设备的文件操作肯定不会和读取正常文件的一样，他们不是读取磁盘，而是读取硬件设备
dentry：目录项，一个指向带有文件路径的dentry结构体指针，我们在操作文件时，一定要知道他的路径，才能进行操作。为了减少读盘次数,内核缓存了目录的树状结构,称为dentry cache,其中每个节点是一 个dentry结构体,只要沿着路径各部分的dentry搜索即可。

现在看下dentry这个结构体指向了什么？
dentry指向了inode，inode是一个包含所有者、文件大小、文件类型和权限位，创建、修改和更新时间等的结构体，保存着从磁盘inode读上来的信息。里面还有两个重要的成员：
分别是inode_opertions和super_block
inode_opertions：是描述文件能进行哪些操作的结构体，他指向了文件操作的内核函数，比如说rm，mkdir，mv等，
super_block：保存着从磁盘分区的超级块读上来的信息，像文件系统类型（比如说是ext2，ext3等），块大小，不同的文件类型，底层的实现是不同的。当然，super_block还有s_root个成员指向了dentry，因为他需要知道文件的根目录被mount 到哪里
file 、dentry、inode 、super_block这几个结构体组成了VFS的核心概念

### redis ziplist redis源码之压缩列表ziplist

[原文](https://blog.csdn.net/qiangzhenyi1207/article/details/80353104)
连续，无序的数据结构。压缩列表是 Redis 为了节约内存而开发的， 由一系列特殊编码的连续内存块组成的顺序型（sequential）数据结构。

### key notification redis数据库通知

[原文](https://www.cnblogs.com/zhangchao-letv/articles/6121635.html)
数据库通知是Redis2.8版本新增加的功能,这个功能可以让客户端通过订阅给定的频道或者模式,来获知数据库中键的变化,以及数据库中命令的执行情况。
举个例子,以下代码展示了客户端如何获取0号数据库中针对message键执行的所有命令:

```
127.0.0.1:6379>SUBSCRIBE__keyspace@0__:message
Reading messages . . . (press Ctrl-C to quit)
1) "subscribe"            //订阅信息
2) "_ _keyspace@0_:message"
3) (integer) 1
1) "message" //执行SET 命令
2) "_ _keyspace@0_: message"
3) "set"
1) "message" //执行EXPIE命令
2) " keyspace@0_:message"
3) "expire"
1) "message" //执行DEL 命令
2) "_ _keyspace@0_: message "
3) "de1"
```
根据发回的通知显示，先后共有SET、EXPlRE 、DEL 三个命令对键message进行了操作。

这一类关注"某个键执行了什么命令"的通知称为键空间通知(key-space-notification),除此之外，还有另一类称为键事件通知(key-event-notification)的通知,它们关注的是"某个命令被什么键执行了" 。
以下是一个键事件通知的例子，代码展示了客户端如何获取0 号数据库中所有执行DEL 命令的键:

```
127.0 . 0.1:6379> SUBSCRIBE_ _keyevent@0_ _:de1
Reading messages. . . (press Ctrl-C to quit)
1) "subcribe"                      //订阅信息
2) "_ _keyevent@0_ _:del"
3) (integer) 1

1) "message"                     //键key执行了DEL命令
2) "keyevent@0_ _:del"
3) "key"

1) "message"                     //键number执行了DEL命令
2) "_ _keyevent@0_ _:del"
3) "number"

1) "message"                     //键message执行了DEL命令
2) "keyevent@0_ _:del"
3) "message"

```

### scan 遍历
SCAN 命令是一个基于游标的迭代器（cursor based iterator）： SCAN 命令每次被调用之后， 都会向用户返回一个新的游标， 用户在下次迭代时需要使用这个新游标作为 SCAN 命令的游标参数， 以此来延续之前的迭代过程。

当 SCAN 命令的游标参数被设置为 0 时， 服务器将开始一次新的迭代， 而当服务器向用户返回值为 0 的游标时， 表示迭代已结束。

### keys 命令

Returns all keys matching pattern.  Redis running on an entry level laptop can scan a 1 million key database in 40 milliseconds.

Warning: consider KEYS as a command that should only be used in production environments with extreme care. It may ruin performance when it is executed against large databases. This command is intended for debugging and special operations, such as changing your keyspace layout. Don't use KEYS in your regular application code. If you're looking for a way to find keys in a subset of your keyspace, consider using SCAN or sets.

### xadd 命令

XADD key ID field string [field string ...]

### zset 与skiplist 数据结构

```C
/* ZSETs use a specialized version of Skiplists */
typedef struct zskiplistNode {
    sds ele;
    double score;
    struct zskiplistNode *backward;
    struct zskiplistLevel {
        struct zskiplistNode *forward;
        unsigned long span;
    } level[];
} zskiplistNode;

typedef struct zskiplist {
    struct zskiplistNode *header, *tail;
    unsigned long length;
    int level;
} zskiplist;

typedef struct zset {
    dict *dict;
    zskiplist *zsl;
} zset;
```

### skiplist 实现zset

[原文](https://blog.csdn.net/weixin_41462047/article/details/81253106)

听到跳表（skiplist）这个名字，既然是list，那么应该跟链表有关。 
跳表是有序链表，但是我们知道，即使对于排过序的链表，我们对于查找还是需要进行通过链表的指针进行遍历的，时间复杂度很高依然是O(n)，这个显然是不能接受的。是否可以像数组那样，通过二分法进行查找呢，但是由于在内存中的存储的不确定性，不能这做。

但是我们可以结合二分法的思想，没错，跳表就是链表与二分法的结合。 
1.链表从头节点到尾节点都是有序的 
2.可以进行跳跃查找（形如二分法），降低时间复杂度

一个有序的链表，我们选取它的一半的节点用来建索引，这样如果插入一个节点，我们比较的次数就减少了一半。这种做法，虽然增加了50%的空间，但是性能提高了一倍。如上图。

既然，我们已经提取了一层节点索引，那么，可以在第一层索引上再提取索引。如下图。

#### 查询

当targetNode->next[i]的值 < 待查找的值时，令targetNode = targetNode->next[i]，targetNode移到第i级的下一个结点； 
当targetNode->next[i]的值 > 待查找的值时，向下降级，i- - ，不改变targetNode； 
当targetNode->next[i]的值 = 待查找的值时，向下降级，i- - ，不改变targetNode。

最后，再次比较targetNode->next[0]和theElement，判断是否找到。 
所以整个运算下来，targetNode是要查找的节点前面那个节点。

#### 插入 
当有2级索引时，新的节点先和2级索引比较，再和1级索引比较，最后和原链表比较，最终插到原链表中。当节点很多时，比较次数是原来的四分之一。

当然，当节点足够多的时候，我们还可以继续加索引，保证每一层索引数是低级索引的一半。当这一层只剩两个节点时，就没有必要再建索引了，因为一个节点没有比较的意义。

当很多节点插入时，上层索引节点已经不够用，我们需要在新节点中选取一部分节点提到上一层，跳表的设计者用“抛硬币”的方法选取节点是否提拔，也就是随机的方式，每个节点有50%概率会提拔。这样虽然不会让索引绝对均匀分布，但也会大体上是均匀的。

综上,插入的步骤：

新节点和各层索引节点逐一比较，确定原链表的插入位置。O（logN）
把索引插入到原链表。O（1）
利用抛硬币的随机方式，决定新节点是否提升为上一级索引。结果为“正”则提升并继续抛硬币，结果为“负”则停止。O（logN）
总体上，跳表插入操作的时间复杂度是O（logN），而这种数据结构所占空间是2N，既空间复杂度是 O（N）。

#### 删除

自上而下，查找第一次出现节点的索引，并逐层找到每一层对应的节点。O（logN）
删除每一层查找到的节点，如果该层只剩下1个节点，删除整个一层（原链表除外）。O（logN）
总体上，跳表删除操作的时间复杂度是O（N）。

#### 应用 
Redis当中的Sorted-set这种有序的集合，正是对于跳表的改进和应用。

相比于二叉查找树，跳表维持结构平衡的成本比较低，完全靠随机。而二叉查找树需要Rebalance来重新调整平衡的结构。 

#### 相比hash 和红黑树优点

如果要实现一个key-value结构，需求的功能有插入、查找、迭代、修改，那么首先Hash表就不是很适合了，因为迭代的时间复杂度比较高；而红黑树的插入很可能会涉及多个结点的旋转、变色操作，因此需要在外层加锁，这无形中降低了它可能的并发度。而SkipList底层是用链表实现的，可以实现为lock free，同时它还有着不错的性能（单线程下只比红黑树略慢），非常适合用来实现我们需求的那种key-value结构。

### zrangebyscore

ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]

```shell
Parse the min-max interval. If one of the values is prefixed
by the "(" character, it's considered "open". For instance
ZRANGEBYSCORE zset (1.5 (2.5 will match min < x < max
ZRANGEBYSCORE zset 1.5 2.5 will instead match min <= x <= max */


127.0.0.1:6379[1]> ZRANGEBYSCORE za 8 (8.1 limit 0 10
1) "first"
2) "s"
127.0.0.1:6379[1]> ZRANGEBYSCORE za 8 9 limit 0 10
1) "first"
2) "s"
3) "s2"
4) "s3"
127.0.0.1:6379[1]> ZRANGEBYSCORE za 8 9 limit 0 10 withscores
1) "first"
2) "8.0800000000000001"
3) "s"
4) "8.0899999999999999"
5) "s2"
6) "8.0999999999999996"
7) "s3"
8) "8.1099999999999994"
```

### setbit

SETBIT key offset value

Sets or clears the bit at offset in the string value stored at key.

The bit is either set or cleared depending on value, which can be either 0 or 1. When key does not exist, a new string value is created. The string is grown to make sure it can hold a bit at offset. The offset argument is required to be greater than or equal to 0, and smaller than 232 (this limits bitmaps to 512MB). When the string at key is grown, added bits are set to 0.

```C
//核心代码
/* SETBIT key offset bitvalue */
void setbitCommand(client *c) {
    robj *o;
    char *err = "bit is not an integer or out of range";
    size_t bitoffset;
    ssize_t byte, bit;
    int byteval, bitval;
    long on;

    if (getBitOffsetFromArgument(c,c->argv[2],&bitoffset,0,0) != C_OK)
        return;

    if (getLongFromObjectOrReply(c,c->argv[3],&on,err) != C_OK)
        return;

    /* Bits can only be set or cleared... */
    if (on & ~1) {
        addReplyError(c,err);
        return;
    }

    if ((o = lookupStringForBitCommand(c,bitoffset)) == NULL) return;

    /* Get current values */
    byte = bitoffset >> 3;
    byteval = ((uint8_t*)o->ptr)[byte];
    bit = 7 - (bitoffset & 0x7);
    bitval = byteval & (1 << bit);

    /* Update byte with new bit value and return original value */
    byteval &= ~(1 << bit);
    byteval |= ((on & 0x1) << bit);
    ((uint8_t*)o->ptr)[byte] = byteval;
}

/* This is an helper function for commands implementations that need to write
 * bits to a string object. The command creates or pad with zeroes the string
 * so that the 'maxbit' bit can be addressed. The object is finally
 * returned. Otherwise if the key holds a wrong type NULL is returned and
 * an error is sent to the client. */
robj *lookupStringForBitCommand(client *c, size_t maxbit) {
    size_t byte = maxbit >> 3;
    robj *o = lookupKeyWrite(c->db,c->argv[1]);

    if (o == NULL) {
        o = createObject(OBJ_STRING,sdsnewlen(NULL, byte+1));
        dbAdd(c->db,c->argv[1],o);
    } else {
        if (checkType(c,o,OBJ_STRING)) return NULL;
        o = dbUnshareStringValue(c->db,c->argv[1],o);
        o->ptr = sdsgrowzero(o->ptr,byte+1);
    }
    return o;
}

//大概总结：
// 1. 对传入的key 首先要保证有一个字符串(redisObject / robj)对象关联
// 2. 再计算一下byte = bitoffset >> 3 这个值，如100 的二进制为01100100，则byte 为01100 = 12。
// 3. 对字符串对象数组，12 这个位置的字节，取出来，并一下本次传入的bitoffset 的低3 位扩展到2 进制的值，如100 的低3 位为100 = 4，所以12 这个位置上，要并一下00010000。代码是 `byteval |= ((on & 0x1) << bit);`
// 4. 查询的时候，取出低三位二进制表示后的那个位的值。
```

### Redis 常见的性能问题和解决方法

1.Master写内存快照，save命令调度rdbSave函数，会阻塞主线程的工作，当快照比较大时对性能影响是非常大的，会间断性暂停服务，所以Master最好不要写内存快照。

2.Master AOF持久化，如果不重写AOF文件，这个持久化方式对性能的影响是最小的，但是AOF文件会不断增大，AOF文件过大会影响Master重启的恢复速度。

3.Master调用BGREWRITEAOF重写AOF文件，AOF在重写的时候会占大量的CPU和内存资源，导致服务load过高，出现短暂服务暂停现象。

下面是我的一个实际项目的情况，大概情况是这样的：一个Master，4个Slave，没有Sharding机制，仅是读写分离，Master负责写入操作和AOF日志备份，AOF文件大概5G，Slave负责读操作，当Master调用BGREWRITEAOF时，Master和Slave负载会突然陡增，Master的写入请求基本上都不响应了，持续了大概5分钟，Slave的读请求过也半无法及时响应，上面的情况本来不会也不应该发生的，是因为以前Master的这个机器是Slave，在上面有一个shell定时任务在每天的上午10点调用BGREWRITEAOF重写AOF文件，后来由于Master机器down了，就把备份的这个Slave切成Master了，但是这个定时任务忘记删除了，就导致了上面悲剧情况的发生，原因还是找了几天才找到的。

将no-appendfsync-on-rewrite的配置设为yes可以缓解这个问题，设置为yes表示rewrite期间对新写操作不fsync，暂时存在内存中，等rewrite完成后再写入。最好是不开启Master的AOF备份功能。

4.Redis主从复制的性能问题，第一次Slave向Master同步的实现是：Slave向Master发出同步请求，Master先dump出rdb文件，然后将rdb文件全量传输给slave，然后Master把缓存的命令转发给Slave，初次同步完成。第二次以及以后的同步实现是：Master将变量的快照直接实时依次发送给各个Slave。不管什么原因导致Slave和Master断开重连都会重复以上过程。Redis的主从复制是建立在内存快照的持久化基础上，只要有Slave就一定会有内存快照发生。虽然Redis宣称主从复制无阻塞，但由于磁盘io的限制，如果Master快照文件比较大，那么dump会耗费比较长的时间，这个过程中Master可能无法响应请求，也就是说服务会中断，对于关键服务，这个后果也是很可怕的。

以上1.2.3.4根本问题的原因都离不开系统io瓶颈问题，也就是硬盘读写速度不够快，主进程 fsync()/write() 操作被阻塞。

5.单点故障问题，由于目前Redis的主从复制还不够成熟，所以存在明显的单点故障问题，这个目前只能自己做方案解决，如：主动复制，Proxy实现Slave对Master的替换等，这个也是Redis作者目前比较优先的任务之一，作者的解决方案思路简单优雅，详情可见 Redis Sentinel design draft http://redis.io/topics/sentinel-spec。

 

总结：

1.Master最好不要做任何持久化工作，包括内存快照和AOF日志文件，特别是不要启用内存快照做持久化。

2.如果数据比较关键，某个Slave开启AOF备份数据，策略为每秒同步一次。

3.为了主从复制的速度和连接的稳定性，Slave和Master最好在同一个局域网内。

4.尽量避免在压力较大的主库上增加从库

5.为了Master的稳定性，主从复制不要用图状结构，用单向链表结构更稳定，即主从关系为：`Master<--Slave1<--Slave2<--Slave3.......`，这样的结构也方便解决单点故障问题，实现Slave对Master的替换，也即，如果Master挂了，可以立马启用Slave1做Master，其他不变。

### 对比redis 和memcache （非常重要）

[原文](https://www.cnblogs.com/JavaBlackHole/p/7726195.html)

综合结论

应该说Memcached和Redis都能很好的满足解决我们的问题，它们性能都很高，总的来说，可以把Redis理解为是对Memcached的拓展，是更加重量级的实现，提供了更多更强大的功能。具体来说：

1.性能上： 
性能上都很出色，具体到细节，由于Redis只使用单核，而Memcached可以使用多核，所以平均每一个核上Redis在存储小数据时比 
Memcached性能更高。而在100k以上的数据中，Memcached性能要高于Redis，虽然Redis最近也在存储大数据的性能上进行优化，但是比起 Memcached，还是稍有逊色。

2.内存空间和数据量大小： 
MemCached可以修改最大内存，采用LRU算法。Redis增加了VM的特性，突破了物理内存的限制。

3.操作便利上： 
MemCached数据结构单一，仅用来缓存数据，而Redis支持更加丰富的数据类型，也可以在服务器端直接对数据进行丰富的操作,这样可以减少网络IO次数和数据体积。

4.可靠性上： 
MemCached不支持数据持久化，断电或重启后数据消失，但其稳定性是有保证的。Redis支持数据持久化和数据恢复，允许单点故障，但是同时也会付出性能的代价。

5.应用场景： 
Memcached：动态系统中减轻数据库负载，提升性能；做缓存，适合多读少写，大数据量的情况（如人人网大量查询用户信息、好友信息、文章信息等）。 
Redis：适用于对读写效率要求都很高，数据处理业务复杂和对安全性要求较高的系统（如新浪微博的计数和微博发布部分系统，对数据安全性、读写要求都很高）。

需要慎重考虑的部分 
1.Memcached单个key-value大小有限，一个value最大只支持1MB，而Redis最大支持512MB 
2.Memcached只是个内存缓存，对可靠性无要求；而Redis更倾向于内存数据库，因此对对可靠性方面要求比较高 
3.从本质上讲，Memcached只是一个单一key-value内存Cache；而Redis则是一个数据结构内存数据库，支持五种数据类型，因此Redis除单纯缓存作用外，还可以处理一些简单的逻辑运算，Redis不仅可以缓存，而且还可以作为数据库用 
4.新版本（3.0）的Redis是指集群分布式，也就是说集群本身均衡客户端请求，各个节点可以交流，可拓展行、可维护性更强大。

#### 补充
在Redis中，并不是所有的数据都一直存储在内存中的，这是和Memcache相比一个最大的区别之一的。
还有这个Redis在很多方面具备数据库的特征的，或者说就是一个数据库系统，而Memcache只是简单的K/V缓存的。
他们的扩展都需要做集群；实现方式：master-slave、Hash的。，不过在100k以上的数据中，Memcache性能要高于Redis的。
如果你对数据持久化和数据同步有所要求，那么推荐你选择Redis的，因为这两个特性Memcache都不具备的。
所以即使你只是希望在升级或者重启系统后缓存数据不会丢失的，选择Redis也是明智的选择得的。
还有虽然这个Redis和Memcache在写入性能上面差别不大的，读取性能上面尤其是批量读取性能上面Memcache更强的。
以上区别说明：memcache还是有可取之处的！！！
Memcache可以利用多核优势的，单实例吞吐量极高的，可以达到几十万QPS,适用于最大程度扛量的的。
他的话只支持简单的key/value数据结构的，不像Redis可以支持丰富的数据类型的。
无法进行持久化，数据不能备份，只能用于缓存使用，而且且重启后数据全部丢失的

### redis 常见35 问题
[原文](https://blog.csdn.net/chinahuyong/article/details/82683155)

### redis watch 

[原文](https://www.jianshu.com/p/ad273642b3bb)
我们常用redis的watch和multi来处理一些涉及并发的操作，redis的watch+multi实际是一种乐观锁，今天我们来分析一下它的实现机制。

```
$key = 'xxxx';
$redis->watch($key);
$redis->multi();
// 更新了key
$redis->set($key);
$flag = $redis->exec();

// 如果事务执行失败返回false
if ($flag === false) {
    
} else {
    
}
```
当客户端A和客户端B同时执行这段代码时候，因为事务的执行是串行的，假设A客户端先于B执行，那么当A执行完成时，会将客户端A从watch了这个key的列表中删除，并且将列表中的所有客户端都设置为CLIENT_DIRTY_CAS，之后当B执行的时候，事务发现B的状态是CLIENT_DIRTY_CAS，便终止事务并返回失败。

```shell
127.0.0.1:6379[1]> watch test2
OK
127.0.0.1:6379[1]> multi
OK
127.0.0.1:6379[1]> incr test2
QUEUED
127.0.0.1:6379[1]> exec
1) (integer) 7
127.0.0.1:6379[1]> watch test2
OK
127.0.0.1:6379[1]> multi
OK
127.0.0.1:6379[1]> incr test2
QUEUED
127.0.0.1:6379[1]> exec
(nil)
127.0.0.1:6379[1]>
```
代码：
```c
/* exec 命令 */
void execCommand(client *c) {
    int j;
    robj **orig_argv;
    int orig_argc;
    struct redisCommand *orig_cmd;
    int must_propagate = 0; /* Need to propagate MULTI/EXEC to AOF / slaves? */
    int was_master = server.masterhost == NULL;
    
    // 未执行multi，则返回
    if (!(c->flags & CLIENT_MULTI)) {
        addReplyError(c,"EXEC without MULTI");
        return;
    }
    
    /*
     * 关键
     * 处理客户端状态 以下两种状态会直接终止事务，不会执行事务队列中的命令
     * 1. CLIENT_DIRTY_CAS => 当因为watch的key被touch了
     * 2. CLIENT_DIRTY_EXEC => 当客户端入队了不存在的命令
     */
    
    /* Check if we need to abort the EXEC because:
     * 1) Some WATCHed key was touched.
     * 2) There was a previous error while queueing commands.
     * A failed EXEC in the first case returns a multi bulk nil object
     * (technically it is not an error but a special behavior), while
     * in the second an EXECABORT error is returned. */
    if (c->flags & (CLIENT_DIRTY_CAS|CLIENT_DIRTY_EXEC)) {
        addReply(c, c->flags & CLIENT_DIRTY_EXEC ? shared.execaborterr :
                                                  shared.nullmultibulk);
        // 
        discardTransaction(c);
        goto handle_monitor;
    }

    /* 执行队列中的命令 */
    // 清空当前客户端中存储的watch了的key，和hash表中客户端node
    unwatchAllKeys(c); /* Unwatch ASAP otherwise we'll waste CPU cycles */
    orig_argv = c->argv;
    orig_argc = c->argc;
    orig_cmd = c->cmd;
    addReplyMultiBulkLen(c,c->mstate.count);
    // 执行队列中的命令
    for (j = 0; j < c->mstate.count; j++) {
    }
}
```

在set 一些值的时候，会把flag 标识为CLIENT_DIRDY_CAS
```c
void setKey(redisDb *db, robj *key, robj *val) {
    if (lookupKeyWrite(db,key) == NULL) {
        dbAdd(db,key,val);
    } else {
        dbOverwrite(db,key,val);
    }
    incrRefCount(val);
    removeExpire(db,key);
    // 看这里👀 标记hash表中所有已经watch这个key的所有客户端状态为CLIENT_DIRTY_CAS
    // 如果我原先的值为1，这里set为1，也会执行这个方法。所以说和值变没变没关系。
    signalModifiedKey(db,key);
}

void signalModifiedKey(redisDb *db, robj *key) {
    touchWatchedKey(db,key);
}

/* 更新hash表中相应客户端的状态为CLIENT_DIRTY_CAS */
void touchWatchedKey(redisDb *db, robj *key) {
    list *clients;
    listIter li;
    listNode *ln;

    if (dictSize(db->watched_keys) == 0) return;
    clients = dictFetchValue(db->watched_keys, key);
    if (!clients) return;

    /* Mark all the clients watching this key as CLIENT_DIRTY_CAS */
    /* Check if we are already watching for this key */
    listRewind(clients,&li);
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);

        c->flags |= CLIENT_DIRTY_CAS;
    }
}
```

### Redis为什么这么快

1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)；

2、数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的；

3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；

4、使用多路I/O复用模型，非阻塞IO；

5、使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求；

以上几点都比较好理解，下边我们针对多路 I/O 复用模型进行简单的探讨：

（1）多路 I/O 复用模型

多路I/O复用模型是利用 select、poll、epoll 可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。

**这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。**采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗），且 Redis 在内存中操作数据的速度非常快，也就是说内存内的操作不会成为影响Redis性能的瓶颈，主要由以上几点造就了 Redis 具有很高的吞吐量。

### 那么为什么Redis是单线程的
我们首先要明白，上边的种种分析，都是为了营造一个Redis很快的氛围！官方FAQ表示，因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了（毕竟采用多线程会有很多麻烦！）。

### cpu 因素
1、我们知道Redis是用"单线程-多路复用IO模型"来实现高性能的内存数据服务的，这种机制避免了使用锁，但是同时这种机制在进行sunion之类的比较耗时的命令时会使redis的并发下降。因为是单一线程，所以同一时刻只有一个操作在进行，所以，耗时的命令会导致并发的下降，不只是读并发，写并发也会下降。而单一线程也只能用到一个CPU核心，所以可以在同一个多核的服务器中，可以启动多个实例，组成master-master或者master-slave的形式，耗时的读命令可以完全在slave进行。

需要改的redis.conf项：

pidfile /var/run/redis/redis_6377.pid  #pidfile要加上端口号
port 6377  #这个是必须改的
logfile /var/log/redis/redis_6377.log #logfile的名称也加上端口号
dbfilename dump_6377.rdb  #rdbfile也加上端口号

2、“我们不能任由操作系统负载均衡，因为我们自己更了解自己的程序，所以，我们可以手动地为其分配CPU核，而不会过多地占用CPU，或是让我们关键进程和一堆别的进程挤在一起。”。
CPU 是一个重要的影响因素，由于是单线程模型，Redis 更喜欢大缓存快速 CPU， 而不是多核

在多核 CPU 服务器上面，Redis 的性能还依赖NUMA 配置和处理器绑定位置。最明显的影响是 redis-benchmark 会随机使用CPU内核。为了获得精准的结果，需要使用固定处理器工具（在 Linux 上可以使用 taskset）。最有效的办法是将客户端和服务端分离到两个不同的 CPU 来高校使用三级缓存。

### 常见线程模型

1、单进程多线程模型：MySQL、Memcached、Oracle（Windows版本）；

2、多进程模型：Oracle（Linux版本）；

3、Nginx有两类进程，一类称为Master进程(相当于管理进程)，另一类称为Worker进程（实际工作进程）。启动方式有两种：

（1）单进程启动：此时系统中仅有一个进程，该进程既充当Master进程的角色，也充当Worker进程的角色。

（2）多进程启动：此时系统有且仅有一个Master进程，至少有一个Worker进程工作。

（3）Master进程主要进行一些全局性的初始化工作和管理Worker的工作；事件处理是在Worker中进行的。

### 主从同步数据

主从同步转移
首先在新服务器上直接进入redis-cli，执行从库配置slaveof 192.168.1.100 6379，这里假设要将192.168.1.100的6379端口的redis服务转移过来。这样就已经开始同步了。通过info可以查看当前服务器是slave。

然后通过info命令查看master_link_status，如果为up，表示同步完成。（在同步过程中，执行查询的时候还是会提示"Redis is loading the dataset in memory"，这属于正常情况.把数据从磁盘文件加载到内存中可能会消耗很长的一段时间。）

最后断开主从关系，在redis-cli命令行下执行slaveof no one提示OK，再通过info查看，该新服务器已经自己变成master了。

<!-- 作者：CRUD老码农 -->
<!-- 链接：https://www.jianshu.com/p/b6b77709561e -->
<!-- 来源：简书 -->
<!-- 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 -->

### 内存调整

属性名	属性说明
used_memory Redis	分配器分配的内存量，也就是实际存储数据的内存总量
used_memory_human	以可读格式返回 Redis 使用的内存总量
used_memory_rss	从操作系统的角度，Redis进程占用的总物理内存
used_memory_peak	内存分配器分配的最大内存，代表used_memory的历史峰值
used_memory_peak_human	以可读的格式显示内存消耗峰值
used_memory_lua	Lua引擎所消耗的内存
mem_fragmentation_ratio	used_memory_rss /used_memory比值，表示内存碎片率
mem_allocator	Redis 所使用的内存分配器。默认: jemalloc

ratio指数`>1`表明有内存碎片，越大表明越多，`<1`表明正在使用虚拟内存，虚拟内存其实就是硬盘，性能比内存低得多，这是应该增强机器的内存以提高性能。一般来说，mem_fragmentation_ratio的数值在1 ~ 1.5之间是比较健康的。

### 遇到命令

#### DUMP key

Serialize the value stored at key in a Redis-specific format and return it to the user. The returned value can be synthesized back into a Redis key using the RESTORE command.

```sh
redis> SET mykey 10
"OK"
redis> DUMP mykey
"\u0000\xC0\n\t\u0000\xBEm\u0006\x89Z(\u0000\n"
redis> 
```

#### DBSIZE
Available since 1.0.0.

Return the number of keys in the currently-selected database.

```sh
10.111.1.54:6381[2]> dbsize
(integer) 20484594
10.111.1.54:6381[2]> dbsize
(integer) 20455432
```

#### wait numreplicas timeout

Available since 3.0.0.

Time complexity: O(1)

This command blocks the current client until all the previous write commands are successfully transferred and acknowledged by at least the specified number of replicas. If the timeout, specified in milliseconds, is reached, the command returns even if the specified number of replicas were not yet reached.

The command will always return the number of replicas that acknowledged the write commands sent before the WAIT command, both in the case where the specified number of replicas are reached, or when the timeout is reached

Return value
Integer reply: The command returns the number of replicas reached by all the writes performed in the context of the current connection.

```sh
> SET foo bar
OK
> WAIT 1 0
(integer) 1
> WAIT 2 1000
(integer) 1
```

In the following example the first call to WAIT does not use a timeout and asks for the write to reach 1 replica. It returns with success. In the second attempt instead we put a timeout, and ask for the replication of the write to two replicas. Since there is a single replica available, after one second WAIT unblocks and returns 1, the number of replicas reached.

### jedis

```java

public <T> T doInRedis(Function<Jedis, T> fn, Jedis jedis) {
    try {
        return fn.apply(jedis);
    } finally {
        jedis.close();
    }
}

// 调用
public void test(){
    String inRedis = doInRedis(jedis1 -> {
        String result = jedis1.get("key1");
        System.out.println("result = " + result);
        return result;
        }, jedis);
    System.out.println("inRedis = " + inRedis);
}
```

### RESP
Redis clients communicate with the Redis server using a protocol called RESP (REdis Serialization Protocol). While the protocol was designed specifically for Redis, it can be used for other client-server software projects.

RESP is a compromise between the following things:

- Simple to implement.
- Fast to parse.
- Human readable.

The RESP protocol was introduced in Redis 1.2, but it became the standard way for talking with the Redis server in Redis 2.0. This is the protocol you should implement in your Redis client.

RESP is actually a serialization protocol that supports the following data types: Simple Strings, Errors, Integers, Bulk Strings and Arrays.

The way RESP is used in Redis as a request-response protocol is the following:

Clients send commands to a Redis server as a RESP Array of Bulk Strings.
The server replies with one of the RESP types according to the command implementation.
In RESP, the type of some data depends on the first byte:

For Simple Strings the first byte of the reply is "+"
For Errors the first byte of the reply is "-"
For Integers the first byte of the reply is ":"
For Bulk Strings the first byte of the reply is "$"
For Arrays the first byte of the reply is "*"
Additionally RESP is able to represent a Null value using a special variation of Bulk Strings or Array as specified later.

In RESP different parts of the protocol are always terminated with "\r\n" (CRLF).


#### RESP Simple Strings: `"+OK\r\n"`

#### RESP Errors: `"-Error message\r\n"`

#### RESP Integers:  `:0\r\n`  `:1000\r\n`

Integer replies are also extensively used in order to return true or false. For instance commands like EXISTS or SISMEMBER will return 1 for true and 0 for false.
Other commands like SADD, SREM and SETNX will return 1 if the operation was actually performed, 0 otherwise.

#### RESP Bulk Strings

Bulk Strings are used in order to represent a single binary safe string up to 512 MB in length.

Bulk Strings are encoded in the following way:

A "$" byte followed by the number of bytes composing the string (a prefixed length), terminated by CRLF.
The actual string data.
A final CRLF.
So the string "foobar" is encoded as follows:

`"$6\r\nfoobar\r\n"`
When an empty string is just:

`"$0\r\n\r\n"`
RESP Bulk Strings can also be used in order to signal non-existence of a value using a special format that is used to represent a Null value. In this special format the length is -1, and there is no data, so a Null is represented as:

`"$-1\r\n"`

#### RESP Arrays
Clients send commands to the Redis server using RESP Arrays. Similarly certain Redis commands returning collections of elements to the client use RESP Arrays are reply type. An example is the LRANGE command that returns elements of a list.

RESP Arrays are sent using the following format:

A * character as the first byte, followed by the number of elements in the array as a decimal number, followed by CRLF.
An additional RESP type for every element of the Array.


So an empty Array is just the following:

`"*0\r\n"`
While an array of two RESP Bulk Strings "foo" and "bar" is encoded as:

`"*2\r\n$3\r\nfoo\r\n$3\r\nbar\r\n"`
As you can see after the *<count>CRLF part prefixing the array, the other data types composing the array are just concatenated one after the other. For example an Array of three integers is encoded as follows:

`"*3\r\n:1\r\n:2\r\n:3\r\n"`
Arrays can contain mixed types, it's not necessary for the elements to be of the same type. For instance, a list of four integers and a bulk string can be encoded as the follows:

```
*5\r\n
:1\r\n
:2\r\n
:3\r\n
:4\r\n
$6\r\n
foobar\r\n
```

#### Sending commands to a Redis Server

- A client sends the Redis server a RESP Array consisting of just Bulk Strings.
- A Redis server replies to clients sending any valid RESP data type as reply.

The client sends the command LLEN mylist in order to get the length of the list stored at key mylist, and the server replies with an Integer reply as in the following example (C: is the client, S: the server).

```
C: *2\r\n
C: $4\r\n
C: LLEN\r\n
C: $6\r\n
C: mylist\r\n

S: :48293\r\n
```

### Pipeline

IMPORTANT NOTE: While the client sends commands using pipelining, the server will be forced to queue the replies, using memory. So if you need to send a lot of commands with pipelining, it is better to send them as batches having a reasonable number, for instance 10k commands, read the replies, and then send another 10k commands again, and so forth. The speed will be nearly the same, but the additional memory used will be at max the amount needed to queue the replies for these 10k commands.

### replication
[原文](https://redis.io/topics/replication)

#### How Redis replication works

Every Redis master has a replication ID: it is a large pseudo random string that marks a given story of the dataset. Each master also takes an offset that increments for every byte of replication stream that it is produced to be sent to replicas, in order to update the state of the replicas with the new changes modifying the dataset. The replication offset is incremented even if no replica is actually connected, so basically every given pair of:

Replication ID, offset
Identifies an exact version of the dataset of a master.

When replicas connect to masters, they use the PSYNC command in order to send their old master replication ID and the offsets they processed so far. This way the master can send just the incremental part needed. However if there is not enough backlog in the master buffers, or if the replica is referring to an history (replication ID) which is no longer known, than a full resynchronization happens: in this case the replica will get a full copy of the dataset, from scratch.

This is how a full synchronization works in more details:

The master starts a background saving process in order to produce an RDB file. At the same time it starts to buffer all new write commands received from the clients. When the background saving is complete, the master transfers the database file to the replica, which saves it on disk, and then loads it into memory. The master will then send all buffered commands to the replica. This is done as a stream of commands and is in the same format of the Redis protocol itself.

You can try it yourself via telnet. Connect to the Redis port while the server is doing some work and issue the SYNC command. You'll see a bulk transfer and then every command received by the master will be re-issued in the telnet session. Actually SYNC is an old protocol no longer used by newer Redis instances, but is still there for backward compatibility: it does not allow partial resynchronizations, so now PSYNC is used instead.

As already said, replicas are able to automatically reconnect when the master-replica link goes down for some reason. If the master receives multiple concurrent replica synchronization requests, it performs a single background save in order to serve all of them.

### redis cluster

[原文](https://redis.io/topics/cluster-tutorial)

So in practical terms, what do you get with Redis Cluster?

- The ability to automatically split your dataset among multiple nodes.
- The ability to continue operations when a subset of the nodes are experiencing failures or are unable to communicate with the rest of the cluster.

The command port and cluster bus port offset is fixed and is always 10000.

Note that for a Redis Cluster to work properly you need, for each node:

- The normal client communication port (usually 6379) used to communicate with clients to be open to all the clients that need to reach the cluster, plus all the other cluster nodes (that use the client port for keys migrations).
- The cluster bus port (the client port + 10000) must be reachable from all the other cluster nodes.

### Redis Cluster configuration parameters

- cluster-enabled <yes/no>: If yes, enables Redis Cluster support in a specific Redis instance. Otherwise the instance starts as a stand alone instance as usual.
- cluster-config-file <filename>: Note that despite the name of this option, this is not a user editable configuration file, but the file where a Redis Cluster node automatically persists the cluster configuration (the state, basically) every time there is a change, in order to be able to re-read it at startup. The file lists things like the other nodes in the cluster, their state, persistent variables, and so forth. Often this file is rewritten and flushed on disk as a result of some message reception.
- cluster-node-timeout <milliseconds>: The maximum amount of time a Redis Cluster node can be unavailable, without it being considered as failing. If a master node is not reachable for more than the specified amount of time, it will be failed over by its slaves. This parameter controls other important things in Redis Cluster. Notably, every node that can't reach the majority of master nodes for the specified amount of time, will stop accepting queries.
- cluster-slave-validity-factor <factor>: If set to zero, a slave will always try to failover a master, regardless of the amount of time the link between the master and the slave remained disconnected. If the value is positive, a maximum disconnection time is calculated as the node timeout value multiplied by the factor provided with this option, and if the node is a slave, it will not try to start a failover if the master link was disconnected for more than the specified amount of time. For example if the node timeout is set to 5 seconds, and the validity factor is set to 10, a slave disconnected from the master for more than 50 seconds will not try to failover its master. Note that any value different than zero may result in Redis Cluster to be unavailable after a master failure if there is no slave able to failover it. In that case the cluster will return back available only when the original master rejoins the cluster.
- cluster-migration-barrier <count>: Minimum number of slaves a master will remain connected with, for another slave to migrate to a master which is no longer covered by any slave. See the appropriate section about replica migration in this tutorial for more information.
- cluster-require-full-coverage <yes/no>: If this is set to yes, as it is by default, the cluster stops accepting writes if some percentage of the key space is not covered by any node. If the option is set to no, the cluster will still serve queries even if only requests about a subset of keys can be processed.
- cluster-allow-reads-when-down <yes/no>: If this is set to no, as it is by default, a node in a Redis Cluster will stop serving all traffic when the cluster is marked as fail, either when a node can't reach a quorum of masters or full coverage is not met. This prevents reading potentially inconsistent data from a node that is unaware of changes in the cluster. This option can be set to yes to allow reads from a node during the fail state, which is useful for applications that want to prioritize read availability but still want to prevent inconsistent writes. It can also be used for when using Redis Cluster with only one or two shards, as it allows the nodes to continue serving writes when a master fails but automatic failover is impossible.

### resharding 

To start a resharding just type:

redis-cli --cluster reshard 127.0.0.1:7000
You only need to specify a single node, redis-cli will find the other nodes automatically.

Currently redis-cli is only able to reshard with the administrator support, you can't just say move 5% of slots from this node to the other one (but this is pretty trivial to implement). So it starts with questions. The first is how much a big resharding do you want to do:

How many slots do you want to move (from 1 to 16384)?
We can try to reshard 1000 hash slots, that should already contain a non trivial amount of keys if the example is still running without the sleep call.

Then redis-cli needs to know what is the target of the resharding, that is, the node that will receive the hash slots. I'll use the first master node, that is, 127.0.0.1:7000, but I need to specify the Node ID of the instance. This was already printed in a list by redis-cli, but I can always find the ID of a node with the following command if I need:

$ redis-cli -p 7000 cluster nodes | grep myself
97a3a64667477371c4479320d683e4c8db5858b1 :0 myself,master - 0 0 0 connected 0-5460
Ok so my target node is 97a3a64667477371c4479320d683e4c8db5858b1.

Now you'll get asked from what nodes you want to take those keys. I'll just type all in order to take a bit of hash slots from all the other master nodes.

After the final confirmation you'll see a message for every slot that redis-cli is going to move from a node to another, and a dot will be printed for every actual key moved from one side to the other.

While the resharding is in progress you should be able to see your example program running unaffected. You can stop and restart it multiple times during the resharding if you want.

At the end of the resharding, you can test the health of the cluster with the following command:

redis-cli --cluster check 127.0.0.1:7000
All the slots will be covered as usual, but this time the master at 127.0.0.1:7000 will have more hash slots, something around 6461.


### CLUSTER REPLICATE node-id
Available since 3.0.0.

Time complexity: O(1)

The command reconfigures a node as a replica of the specified master. If the node receiving the command is an empty master, as a side effect of the command, the node role is changed from master to replica.

Once a node is turned into the replica of another master node, there is no need to inform the other cluster nodes about the change: heartbeat packets exchanged between nodes will propagate the new configuration automatically.

A replica will always accept the command, assuming that:

The specified node ID exists in its nodes table.
The specified node ID does not identify the instance we are sending the command to.
The specified node ID is a master.

### 配置文件

appendonly yes                  #在/data/redis/目录生成appendonly.aof文件，将每一次写操作请求都追加到appendonly.aof 文件中

### 恢复

1. psync 同步rdb 和aof 文件，一直同步aof 直到取消
2. 先从rdb 恢复数据
3. 读取aof，恢复

### No reachable node in cluster

集群模式要打开

```sh
➜  ~ redis-cli -c -p 7001
127.0.0.1:7001> cluster info
cluster_state:fail
cluster_slots_assigned:0
cluster_slots_ok:0
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:1
cluster_size:0
cluster_current_epoch:0
cluster_my_epoch:0
cluster_stats_messages_sent:0
cluster_stats_messages_received:0
127.0.0.1:7001> exit

➜  create-cluster ./create-cluster create
>>> Performing hash slots allocation on 6 nodes...
Master[0] -> Slots 0 - 5460
Master[1] -> Slots 5461 - 10922
Master[2] -> Slots 10923 - 16383
Adding replica 127.0.0.1:7005 to 127.0.0.1:7001
Adding replica 127.0.0.1:7006 to 127.0.0.1:7002
Adding replica 127.0.0.1:7004 to 127.0.0.1:7003
>>> Trying to optimize slaves allocation for anti-affinity
[WARNING] Some slaves are in the same host as their master
M: 6ebf203431654080c971d1d2056e1bfc3823b073 127.0.0.1:7001
   slots:[0-5460] (5461 slots) master
M: 5c50d86191663883ec10e25bcc488e49ebed4b7e 127.0.0.1:7002
   slots:[5461-10922] (5462 slots) master
M: 18669db5e03ddeccc5b739793b64816d52124d75 127.0.0.1:7003
   slots:[10923-16383] (5461 slots) master
S: ecfa79d2b273fd27fbfe1c89244ae6926372a2c5 127.0.0.1:7004
   replicates 18669db5e03ddeccc5b739793b64816d52124d75
S: e7fca47ed70cdff066dd98f16c4046f32346c042 127.0.0.1:7005
   replicates 6ebf203431654080c971d1d2056e1bfc3823b073
S: ca9dabc53c565e15ab7e0475b5cb2c0839d21210 127.0.0.1:7006
   replicates 5c50d86191663883ec10e25bcc488e49ebed4b7e
Can I set the above configuration? (type 'yes' to accept): yes
>>> Nodes configuration updated
>>> Assign a different config epoch to each node
>>> Sending CLUSTER MEET messages to join the cluster
Waiting for the cluster to join
.
>>> Performing Cluster Check (using node 127.0.0.1:7001)
M: 6ebf203431654080c971d1d2056e1bfc3823b073 127.0.0.1:7001
   slots:[0-5460] (5461 slots) master
   1 additional replica(s)
S: ca9dabc53c565e15ab7e0475b5cb2c0839d21210 127.0.0.1:7006
   slots: (0 slots) slave
   replicates 5c50d86191663883ec10e25bcc488e49ebed4b7e
S: ecfa79d2b273fd27fbfe1c89244ae6926372a2c5 127.0.0.1:7004
   slots: (0 slots) slave
   replicates 18669db5e03ddeccc5b739793b64816d52124d75
S: e7fca47ed70cdff066dd98f16c4046f32346c042 127.0.0.1:7005
   slots: (0 slots) slave
   replicates 6ebf203431654080c971d1d2056e1bfc3823b073
M: 5c50d86191663883ec10e25bcc488e49ebed4b7e 127.0.0.1:7002
   slots:[5461-10922] (5462 slots) master
   1 additional replica(s)
M: 18669db5e03ddeccc5b739793b64816d52124d75 127.0.0.1:7003
   slots:[10923-16383] (5461 slots) master
   1 additional replica(s)
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.



➜  ~ redis-cli -c -p 7001
127.0.0.1:7001> cluster info
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:6
cluster_size:3
cluster_current_epoch:6
cluster_my_epoch:1
cluster_stats_messages_ping_sent:40
cluster_stats_messages_pong_sent:42
cluster_stats_messages_sent:82
cluster_stats_messages_ping_received:37
cluster_stats_messages_pong_received:40
cluster_stats_messages_meet_received:5
cluster_stats_messages_received:82
127.0.0.1:7001> exit
```

### pub/sub

```sh
➜  docker_images redis-cli
127.0.0.1:6379> SUBSCRIBE d
Reading messages... (press Ctrl-C to quit)
1) "subscribe"
2) "d"
3) (integer) 1
1) "message"
2) "d"
3) "abcdefg"

127.0.0.1:6379> PUBLISH d abcdefg
(integer) 1


➜  docker_images redis-cli
127.0.0.1:6379> SUBSCRIBE d e
Reading messages... (press Ctrl-C to quit)
1) "subscribe"
2) "d"
3) (integer) 1
1) "subscribe"
2) "e"
3) (integer) 2
1) "message"
2) "d"
3) "abcdefg"
1) "message"
2) "e"
3) "gfedcba"

127.0.0.1:6379> PUBLISH d abcdefg
(integer) 1
127.0.0.1:6379> PUBLISH e gfedcba
(integer) 1

```

####  subscribe

SUBSCRIBE channel [channel ...]
Available since 2.0.0.

Time complexity: O(N) where N is the number of channels to subscribe to.

Subscribes the client to the specified channels.

Once the client enters the subscribed state it is not supposed to issue any other commands, except for additional SUBSCRIBE, PSUBSCRIBE, UNSUBSCRIBE, PUNSUBSCRIBE, PING, RESET and QUIT commands.

#### psubscribe

PSUBSCRIBE pattern [pattern ...]
Available since 2.0.0.

Time complexity: O(N) where N is the number of patterns the client is already subscribed to.

Subscribes the client to the given patterns.

Supported glob-style patterns:

h?llo subscribes to hello, hallo and hxllo
h*llo subscribes to hllo and heeeello
h[ae]llo subscribes to hello and hallo, but not hillo
Use \ to escape special characters if you want to match them verbatim.

#### PUNSUBSCRIBE

PUNSUBSCRIBE [pattern [pattern ...]]
Available since 2.0.0.

Time complexity: O(N+M) where N is the number of patterns the client is already subscribed and M is the number of total patterns subscribed in the system (by any client).

Unsubscribes the client from the given patterns, or from all of them if none is given.

When no patterns are specified, the client is unsubscribed from all the previously subscribed patterns. In this case, a message for every unsubscribed pattern will be sent to the client.

#### message format 
Format of pushed messages
A message is a Array reply with three elements.

The first element is the kind of message:

subscribe: means that we successfully subscribed to the channel given as the second element in the reply. The third argument represents the number of channels we are currently subscribed to.

unsubscribe: means that we successfully unsubscribed from the channel given as second element in the reply. The third argument represents the number of channels we are currently subscribed to. When the last argument is zero, we are no longer subscribed to any channel, and the client can issue any kind of Redis command as we are outside the Pub/Sub state.

message: it is a message received as result of a PUBLISH command issued by another client. The second element is the name of the originating channel, and the third argument is the actual message payload.

#### timing of expired events

Timing of expired events
Keys with a time to live associated are expired by Redis in two ways:

When the key is accessed by a command and is found to be expired.
Via a background system that looks for expired keys in the background, incrementally, in order to be able to also collect keys that are never accessed.
The expired events are generated when a key is accessed and is found to be expired by one of the above systems, as a result there are no guarantees that the Redis server will be able to generate the expired event at the time the key time to live reaches the value of zero.

If no command targets the key constantly, and there are many keys with a TTL associated, there can be a significant delay between the time the key time to live drops to zero, and the time the expired event is generated.

Basically expired events are generated when the Redis server deletes the key and not when the time to live theoretically reaches the value of zero.

#### config

127.0.0.1:6379> config set notify-keyspace-events KExg
OK
127.0.0.1:6379> config get *ev*
1) "lazyfree-lazy-eviction"
2) "no"
3) "loglevel"
4) "notice"
5) "notify-keyspace-events"
6) "KE"


### --bigkeys

bigkeys对问题的排查非常方便，但是在使用它时候也有几点需要注意:

建议在从节点执行，因为--bigkeys也是通过scan完成的。
建议在节点本机执行，这样可以减少网络开销。
如果没有从节点，可以使用--i参数，例如(--i 0.1 代表100毫秒执行一次)
--bigkeys只能计算每种数据结构的top1，如果有些数据结构非常多的bigkey，也搞不定，毕竟不是自己写的东西嘛

### 清理redis空间
[原文](https://zhuanlan.zhihu.com/p/259719544)

